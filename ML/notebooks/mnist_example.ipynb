{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf46d903",
   "metadata": {},
   "source": [
    "# MNIST dataset classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d972ff1",
   "metadata": {},
   "source": [
    "## Preliminary operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83e33e",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eefd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ede00",
   "metadata": {},
   "source": [
    "### Download file and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c219694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-28 08:59:52--  https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "Caricato certificato CA \"/etc/ssl/certs/ca-certificates.crt\"\n",
      "\n",
      "Risoluzione di s3.amazonaws.com (s3.amazonaws.com)... 52.217.229.184\n",
      "Connessione a s3.amazonaws.com (s3.amazonaws.com)|52.217.229.184|:443... connesso.\n",
      "Richiesta HTTP inviata, in attesa di risposta... 200 OK\n",
      "Lunghezza: 11490434 (11M) [application/octet-stream]\n",
      "Salvataggio in: «mnist.npz.3»\n",
      "\n",
      "mnist.npz.3         100%[===================>]  10,96M   224KB/s    in 31s     \n",
      "\n",
      "2022-04-28 09:00:24 (361 KB/s) - «mnist.npz.3» salvato [11490434/11490434]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/img-datasets/mnist.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dataset is downloaded as .npz file.\n",
    "When an .npz file is loaded with np.load()\n",
    "the result is a dictionary-like object whose\n",
    "elements are acccessible with data[\"label\"].\n",
    "The files attribute returns a list with\n",
    "the possible labels.\n",
    "Example:\n",
    "\n",
    "> data = np.load(\"file.npz\")\n",
    "> print(data.files)\n",
    "\"\"\"\n",
    "\n",
    "def return_ds():\n",
    "    path = \"mnist.npz\"\n",
    "\n",
    "    ds = np.load(path)\n",
    "    print(f\"loaded dataset with labels {ds.files}\\n\\\n",
    "Subsets are returned in the following order:\\n\\\n",
    "x_train, x_test, y_train, y_test\\n\")\n",
    "\n",
    "    return ds[ds.files[1]], ds[ds.files[0]], ds[ds.files[2]], ds[ds.files[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505c021",
   "metadata": {},
   "source": [
    "## With PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf0954",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "709a6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d4ad8",
   "metadata": {},
   "source": [
    "### Normalize data, create dataset, create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "31874cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset with labels ['x_test', 'x_train', 'y_train', 'y_test']\n",
      "Subsets are returned in the following order:\n",
      "x_train, x_test, y_train, y_test\n",
      "\n",
      "Training inputs shape: (60000, 28, 28)\n",
      "Validation inputs shape: (10000, 28, 28)\n",
      "Training outputs shape: (60000,)\n",
      "Validation outputs shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "x_train, x_test, y_train, y_test = return_ds()\n",
    "\n",
    "print(f\"Training inputs shape: {x_train.shape}\")\n",
    "print(f\"Validation inputs shape: {x_test.shape}\")\n",
    "print(f\"Training outputs shape: {y_train.shape}\")\n",
    "print(f\"Validation outputs shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "d8e164e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2679/745248417.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_train, x_test, y_train, y_test = map(\n"
     ]
    }
   ],
   "source": [
    "# Normalize and convert to tensor\n",
    "x_train = (x_train - x_train.mean())/x_train.std()\n",
    "x_test = (x_test - x_test.mean())/x_test.std()\n",
    "\n",
    "x_train, x_test, y_train, y_test = map(\n",
    "                                        torch.tensor,\n",
    "                                        (x_train, x_test, y_train, y_test)\n",
    "                                        )\n",
    "\n",
    "# we need the following when converting from numpy\n",
    "# otherwise pytorch will complain when training a model\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "x_train = x_train.float()\n",
    "x_test = x_test.float()\n",
    "y_train = y_train.long()\n",
    "y_test = y_test.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "abe22073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloader\n",
    "batch_sz = 100\n",
    "\n",
    "cnn_train_ds = TensorDataset(x_train.unsqueeze(1), y_train)\n",
    "cnn_valid_ds = TensorDataset(x_test.unsqueeze(1), y_test)\n",
    "mlp_train_ds = TensorDataset(x_train.reshape(-1, 28*28), y_train)\n",
    "mlp_valid_ds = TensorDataset(x_test.reshape(-1, 28*28), y_test)\n",
    "\n",
    "cnn_train_dl = DataLoader(cnn_train_ds, batch_size=batch_sz, shuffle=True)\n",
    "cnn_valid_dl = DataLoader(cnn_valid_ds, batch_size=batch_sz*4)\n",
    "mlp_train_dl = DataLoader(mlp_train_ds, batch_size=batch_sz, shuffle=True)\n",
    "mlp_valid_dl = DataLoader(mlp_valid_ds, batch_size=batch_sz*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76881a1",
   "metadata": {},
   "source": [
    "### Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "id": "4f02ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for fitting model and evaluating\n",
    "\n",
    "def step(model: torch.nn.Module,\n",
    "        xb: torch.Tensor,\n",
    "        yb: torch.Tensor,\n",
    "        loss_func: callable,\n",
    "        opt: torch.optim.Optimizer) -> torch.Tensor:\n",
    "        \n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def fit(model: torch.nn.Module, \n",
    "        loss_func: callable,\n",
    "        train_dl: torch.utils.data.DataLoader,\n",
    "        val_dl: torch.utils.data.DataLoader,\n",
    "        epochs: int,\n",
    "        opt: torch.optim.Optimizer) -> None:\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for train_xb, train_yb in train_dl:\n",
    "            loss = step(model, train_xb, train_yb, loss_func, opt)\n",
    "\n",
    "        print(f\"Training loss at epoch {epoch}: {loss:.2f}\")\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = 0\n",
    "            for val_xb, val_yb in val_dl:\n",
    "                loss += loss_func(model(val_xb), val_yb)\n",
    "            loss = loss/len(val_dl)\n",
    "            print(f\"Validation loss at epoch {epoch}: {loss:.2f}\\n\")\n",
    "\n",
    "\n",
    "def eval_single_input(model: torch.nn.Module,\n",
    "                        input: torch.Tensor) -> torch.Tensor:\n",
    "    out = model(input)\n",
    "    return torch.argmax(out)\n",
    "\n",
    "\n",
    "def eval(model: torch.nn.Module,\n",
    "        test_x: torch.Tensor,\n",
    "        test_y: torch.Tensor) -> float:\n",
    "    n_success = 0\n",
    "    for n, (x, y) in enumerate(zip(test_x, test_y)):\n",
    "        out_model = eval_single_input(model, x)\n",
    "        n_success += (out_model == y)\n",
    "\n",
    "    return n_success.item()/n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b0981",
   "metadata": {},
   "source": [
    "### Define and train an MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "id": "e14c0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an MLP\n",
    "# might add input and output types to methods of the class\n",
    "# also, arguments that specify how many inputs the network must expect might be good\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(784, 20)\n",
    "        self.layer2 = nn.Linear(20,15)\n",
    "        self.layer3 = nn.Linear(15, 10)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.layer1(input))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        out = self.layer3(x)\n",
    "        # out = F.log_softmax(self.layer3(x), dim = 1) #we use this if we choose nll_loss\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "id": "e7267c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a cross entropy loss (or a nll loss)\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "# loss_func = F.nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "id": "607039ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a model and an optimizer\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "mlp_model = MLP()\n",
    "optim = opt.SGD(mlp_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "db2a6753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3079, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3166,  0.0041, -0.1941,  0.1843,  0.1683,  0.0991,  0.2958, -0.1189,\n",
       "         -0.2089,  0.0291]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 944,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "IMPORTANT\n",
    "The model expects a rank-2 tensor as input, because the first dimension\n",
    "corresponds to the batch size.\n",
    "\"\"\"\n",
    "\n",
    "# get a batch\n",
    "xb, yb = next(iter(mlp_train_dl))\n",
    "\n",
    "# evaluate loss for this batch\n",
    "print(loss_func(mlp_model(xb), yb))\n",
    "\n",
    "# compute output for a single element of the batch\n",
    "# note that I need to unsqueeze to get a (1, 784) input to pass to the model\n",
    "mlp_model(xb[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "id": "2674b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 0: 0.31\n",
      "Validation loss at epoch 0: 0.22\n",
      "\n",
      "Training loss at epoch 1: 0.09\n",
      "Validation loss at epoch 1: 0.18\n",
      "\n",
      "Training loss at epoch 2: 0.23\n",
      "Validation loss at epoch 2: 0.17\n",
      "\n",
      "Training loss at epoch 3: 0.18\n",
      "Validation loss at epoch 3: 0.17\n",
      "\n",
      "Training loss at epoch 4: 0.09\n",
      "Validation loss at epoch 4: 0.14\n",
      "\n",
      "Training loss at epoch 5: 0.20\n",
      "Validation loss at epoch 5: 0.15\n",
      "\n",
      "Training loss at epoch 6: 0.12\n",
      "Validation loss at epoch 6: 0.16\n",
      "\n",
      "Training loss at epoch 7: 0.17\n",
      "Validation loss at epoch 7: 0.18\n",
      "\n",
      "Training loss at epoch 8: 0.15\n",
      "Validation loss at epoch 8: 0.15\n",
      "\n",
      "Training loss at epoch 9: 0.14\n",
      "Validation loss at epoch 9: 0.16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "n_epochs = 10\n",
    "\n",
    "fit(mlp_model, loss_func, mlp_train_dl, mlp_valid_dl, n_epochs, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "id": "1e9cd0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb500a545b0>"
      ]
     },
     "execution_count": 950,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAANr0lEQVR4nO3dbYxc5XnG8evCWZvEvNSOa9d1IBBeStyqccpiILQVlZUUUFqgFQ3+gIyKatqGClRSldJKIDWRaFIgURulNcWNSSkRiFD8waGxrKgmagpeU9fYdQgvdcCx45cQZOMEs5i7H/YQLWbn2WXOmTmD7/9PWs3MuefMuXXsa8/sPGfO44gQgKPfMW03AKA/CDuQBGEHkiDsQBKEHUjiXf3c2HTPiGM1s5+bBFJ5RQf1ahzyRLVaYbd9kaQvSJom6Z8i4rbS84/VTJ3rJXU2CaDgsVjXsdb123jb0yR9UdLFkhZKWmp7YbevB6C36vzNvljSMxHxXES8Kumrki5tpi0ATasT9gWSXhj3eEe17E1sL7c9YntkVIdqbA5AHXXCPtGHAG859zYiVkTEcEQMD2lGjc0BqKNO2HdIOmnc4/dJ2lmvHQC9UifsGySdYftU29MlXSlpdTNtAWha10NvEfGa7esk/bvGht5WRsTWxjoD0Kha4+wRsUbSmoZ6AdBDnC4LJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLWlM22t0s6IOmwpNciYriJpgA0r1bYK78REfsaeB0APcTbeCCJumEPSd+wvdH28omeYHu57RHbI6M6VHNzALpV9238BRGx0/ZcSWttfyci1o9/QkSskLRCkk7w7Ki5PQBdqnVkj4id1e0eSQ9JWtxEUwCa13XYbc+0ffwb9yV9TNKWphoD0Kw6b+PnSXrI9huv868R8UgjXQFoXNdhj4jnJH2owV4A9BBDb0AShB1IgrADSRB2IAnCDiTRxBdh0LKXrjq/Y+3g5fuL684YGq217Y+fvLVYf/4nszvWNu+dX1x37l+Uj0Wvb/lOsY4348gOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzj4Atn+m8zi5JK296nPF+snv2tRgN2/Pp/edVayff+KzHWv/fPKjxXV/9PUfF+vDD/5psX76Df9VrGfDkR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcvQ8OfOK8Yv3xZXcU6ycec1yxfsOuzpPnbvz02cV1j9+yt1ifTLyws1j39Lkda/82r7xf9n1+WrH+7d+9vVg/zzd2rJ1xfb4xeI7sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+wNOObYY4v1RZ/aVKwf5xnF+mn3/2Gx/guf6fyd8ffsfay47uFitQGvvNK5tr98Tfs5S08o1n+wuTwO/5FzOl9Xvt7ZBe9Mkx7Zba+0vcf2lnHLZttea/vp6nZWb9sEUNdU3sZ/WdJFRyy7SdK6iDhD0rrqMYABNmnYI2K9pBePWHyppFXV/VWSLmu2LQBN6/YDunkRsUuSqtuOJ0DbXm57xPbIqA51uTkAdfX80/iIWBERwxExPKTyB1EAeqfbsO+2PV+Sqts9zbUEoBe6DftqScuq+8skPdxMOwB6ZdJxdtv3SbpQ0hzbOyTdIuk2SffbvkbS85Ku6GWTg+6pf1xYrH99wcpi/eyNVxbrk13/vOdj5S159s9+sVj/j4PfK9Z/+NF6c88fbSYNe0Qs7VBa0nAvAHqI02WBJAg7kARhB5Ig7EAShB1Igq+4NuCmxY8U6/sOHyzW5/x1zjMLf3LZ4mJ9y+//fbH+V3vKl8l+/WB5v2fDkR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcvQ+eGS1falqPP9mfRlpwzC+f1bF2y+13F9cdcvlS0fdvOKdYP1MbivVsOLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMs/fB6UOFaYslvXzFucX6cQ+Up11u0w+u/0ix/rk/uatjbcm7yxfB/rsfvb9Y/+CnnirWj9ZLbHeLIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4ewM2HiiPBy8/cWexvubOzxfrv3PtJ4r17bveW6zX8dlzHyzWPz6z/J3xGR7qett3rP/NYv3M/Y93/doZTXpkt73S9h7bW8Ytu9X2921vqn4u6W2bAOqaytv4L0u6aILld0bEoupnTbNtAWjapGGPiPWSXuxDLwB6qM4HdNfZ3ly9zZ/V6Um2l9sesT0yqkM1Ngegjm7D/iVJp0laJGmXpNs7PTEiVkTEcEQMDynnBIbAIOgq7BGxOyIOR8Trku6SVJ6OE0Drugq77fnjHl4uaUun5wIYDJOOs9u+T9KFkubY3iHpFkkX2l4kKSRtl3Rt71ocfC/81vHF+pJ7f7tYX7dwda26FpbLdTx+aLRYP2vNHxfrt/zawx1rV5+wp7juBx7gG+lNmjTsEbF0gsXlq/sDGDicLgskQdiBJAg7kARhB5Ig7EASfMW1AYd3l4eQhi6eXqyfe+UfFeuOKNb3nN+5Pv2H5d/nJz/y42J9aNdLxfqZ/1f+iuvq9R/qWLv6hLXFdadv+G6x/nqxiiNxZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhn74MYfbVY/5mvfLvW65/4L7VWL3qtdy89+bYXnV6sH/Pof/epk6MDR3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJxdgys3YvfXazPf7RPjRwlOLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUhi0rDbPsn2N21vs73V9vXV8tm219p+urqd1ft2AXRrKkf21yTdGBEflHSepE/aXijpJknrIuIMSeuqxwAG1KRhj4hdEfFEdf+ApG2SFki6VNKq6mmrJF3Wox4BNOBt/c1u+xRJH5b0mKR5EbFLGvuFIGluh3WW2x6xPTKqQzXbBdCtKYfd9nGSHpR0Q0Tsn+p6EbEiIoYjYnhIM7rpEUADphR220MaC/q9EfG1avFu2/Or+nxJ5alMAbRq0q+42rakuyVti4g7xpVWS1om6bbq9uGedIi03rObSZmbNJXvs18g6SpJT9reVC27WWMhv9/2NZKel3RFTzoE0IhJwx4R35LkDuUlzbYDoFc4gw5IgrADSRB2IAnCDiRB2IEkuJQ0Btash7cW64zCvz0c2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZUcu0eRNejeyn/uHUhzrWvvjSWcV14xUuY9YkjuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7Kjl4DmnFOtzp83sWHtgx9nFdWeMbu+iI3TCkR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkpjK/OwnSbpH0s9p7FLdKyLiC7ZvlfQHkvZWT705Itb0qlEMpr2Luj9VY9+6ny/WF2h716+Nt5rKv9Rrkm6MiCdsHy9po+21Ve3OiPjb3rUHoClTmZ99l6Rd1f0DtrdJWtDrxgA06239zW77FEkflvRYteg625ttr7Q9q8M6y22P2B4ZFZcZAtoy5bDbPk7Sg5JuiIj9kr4k6TRJizR25L99ovUiYkVEDEfE8JBm1O8YQFemFHbbQxoL+r0R8TVJiojdEXE4Il6XdJekxb1rE0Bdk4bdtiXdLWlbRNwxbvn8cU+7XNKW5tsD0JSpfBp/gaSrJD1pe1O17GZJS20vkhSStku6tgf94R3unv1zOtYW/M1/9rETTOXT+G9J8gQlxtSBdxDOoAOSIOxAEoQdSIKwA0kQdiAJwg4k4Yjo28ZO8Ow410v6tj0gm8dinfbHixMNlXNkB7Ig7EAShB1IgrADSRB2IAnCDiRB2IEk+jrObnuvpO+NWzRH0r6+NfD2DGpvg9qXRG/darK390fEz05U6GvY37JxeyQihltroGBQexvUviR661a/euNtPJAEYQeSaDvsK1refsmg9jaofUn01q2+9Nbq3+wA+qftIzuAPiHsQBKthN32Rbafsv2M7Zva6KET29ttP2l7k+2RlntZaXuP7S3jls22vdb209XthHPstdTbrba/X+27TbYvaam3k2x/0/Y221ttX18tb3XfFfrqy37r+9/stqdJ+q6kj0raIWmDpKUR8b99baQD29slDUdE6ydg2P51SS9Luicifqla9llJL0bEbdUvylkR8ecD0tutkl5uexrvarai+eOnGZd0maSr1eK+K/T1e+rDfmvjyL5Y0jMR8VxEvCrpq5IubaGPgRcR6yW9eMTiSyWtqu6v0th/lr7r0NtAiIhdEfFEdf+ApDemGW913xX66os2wr5A0gvjHu/QYM33HpK+YXuj7eVtNzOBeRGxSxr7zyNpbsv9HGnSabz76Yhpxgdm33Uz/XldbYR9outjDdL43wUR8SuSLpb0yertKqZmStN498sE04wPhG6nP6+rjbDvkHTSuMfvk7SzhT4mFBE7q9s9kh7S4E1FvfuNGXSr2z0t9/NTgzSN90TTjGsA9l2b05+3EfYNks6wfart6ZKulLS6hT7ewvbM6oMT2Z4p6WMavKmoV0taVt1fJunhFnt5k0GZxrvTNONqed+1Pv15RPT9R9IlGvtE/llJf9lGDx36+oCk/6l+trbdm6T7NPa2blRj74iukfReSeskPV3dzh6g3r4i6UlJmzUWrPkt9farGvvTcLOkTdXPJW3vu0JffdlvnC4LJMEZdEAShB1IgrADSRB2IAnCDiRB2IEkCDuQxP8DSKcImAHjNAYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = x_test[105]\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "id": "f96f23a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This digit is a 9 according to my model.\n"
     ]
    }
   ],
   "source": [
    "print(f\"This digit is a \\\n",
    "{eval_single_input(mlp_model, im.view(-1, 28*28)).item()} \\\n",
    "according to my model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "id": "b6f57a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy over validation set: 95.44%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation accuracy over validation set: \\\n",
    "{eval(mlp_model, x_test.view(-1, 784), y_test.view(-1,1))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f341d",
   "metadata": {},
   "source": [
    "### Define and train a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "id": "fcbfb0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a CNN\n",
    "# using several channels internally noticeably improves performance (obviously training slows down)\n",
    "\n",
    "# might add input and output types to methods of the class\n",
    "# also, arguments that specify how many inputs the network must expect might be good\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 12, kernel_size=4, padding=(2,2), padding_mode=\"zeros\")\n",
    "        self.conv2 = nn.Conv2d(12, 9, kernel_size=3, padding=(1,1), padding_mode=\"zeros\")\n",
    "        self.conv3 = nn.Conv2d(9, 9, kernel_size=3, padding=(1,1), padding_mode=\"zeros\")\n",
    "        self.dense1 = nn.Linear(49*9, 10)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(input))\n",
    "        x = F.avg_pool2d(x, kernel_size=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.avg_pool2d(x, kernel_size=2)\n",
    "        x = x.reshape(-1, 49*9)\n",
    "        out = self.dense1(x)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "id": "204cac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a loss\n",
    "\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "id": "bd83bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model and optimizer\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "cnn_model = CNN()\n",
    "optim = opt.SGD(cnn_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "id": "b436b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb has shape torch.Size([100, 1, 28, 28])\n",
      "\n",
      "sample from xb has shape torch.Size([1, 1, 28, 28])\n",
      "\n",
      "Output from a batch sample: tensor([[ 0.0795, -0.0913,  0.0535, -0.0245, -0.0832, -0.0071,  0.0633, -0.0404,\n",
      "         -0.0319, -0.0211]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(cnn_train_dl))\n",
    "sample = xb[0].unsqueeze(0)\n",
    "\n",
    "print(f\"xb has shape {xb.shape}\\n\")\n",
    "print(f\"sample from xb has shape {sample.shape}\\n\")\n",
    "\n",
    "print(f\"Output from a batch sample: {cnn_model(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "id": "295ffffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 0: 0.18\n",
      "Validation loss at epoch 0: 0.11\n",
      "\n",
      "Training loss at epoch 1: 0.01\n",
      "Validation loss at epoch 1: 0.07\n",
      "\n",
      "Training loss at epoch 2: 0.01\n",
      "Validation loss at epoch 2: 0.06\n",
      "\n",
      "Training loss at epoch 3: 0.04\n",
      "Validation loss at epoch 3: 0.05\n",
      "\n",
      "Training loss at epoch 4: 0.06\n",
      "Validation loss at epoch 4: 0.05\n",
      "\n",
      "Training loss at epoch 5: 0.02\n",
      "Validation loss at epoch 5: 0.04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 6\n",
    "\n",
    "fit(cnn_model, loss_func, cnn_train_dl, cnn_valid_dl, n_epochs, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "id": "c029316a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1, 1, 28, 28])\n",
      "Validation accuracy over validation set: 98.55%\n"
     ]
    }
   ],
   "source": [
    "x_samples = x_test.unsqueeze(1).unsqueeze(1)\n",
    "y_samples = y_test.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "print(x_samples.shape)\n",
    "\n",
    "print(f\"Validation accuracy over validation set: \\\n",
    "{eval(cnn_model, x_samples, y_samples)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "id": "ae276fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb500a93970>"
      ]
     },
     "execution_count": 960,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = x_test[105]\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "id": "7fe67ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "This digit is a 9 according to my model.\n"
     ]
    }
   ],
   "source": [
    "print(f\"This digit is a \\\n",
    "{eval_single_input(cnn_model, im.unsqueeze(0).unsqueeze(0)).item()} \\\n",
    "according to my model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79457098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
